{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "News Prediction.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM32nibx4lyF8YwQR9rDynm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dkurbatovv/Python/blob/main/News_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5u1yVDSTUca"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm, trange\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_table('train.tsv', sep = '\\t')\n",
        "df_test = pd.read_table('test.tsv', sep = '\\t', header = None)"
      ],
      "metadata": {
        "id": "pB1A5qumTXqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train"
      ],
      "metadata": {
        "id": "o8BYjv8ZTgKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.shape"
      ],
      "metadata": {
        "id": "ZaNJo-o8TzkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (12,10))\n",
        "sns.countplot(df_train.is_fake)"
      ],
      "metadata": {
        "id": "4IYaacMFT5UD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Предобработка***"
      ],
      "metadata": {
        "id": "PR-3tpjHV0nv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def remove_punctuation(text):\n",
        "  return \"\".join([ch if ch not in string.punctuation else ' ' for ch in text])\n",
        "\n",
        "\n",
        "def remove_numbers(text):\n",
        "  return \"\".join([i if not i.isdigit() else ' ' for i in text])  \n",
        "\n",
        "\n",
        "import re\n",
        "def remove_multiple_spaces(text):  \n",
        "  return re.sub(r'\\s+', ' ', text, flags=re.I)\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import *\n",
        "nltk.download('all')\n",
        "from nltk.corpus import stopwords\n",
        "from pymystem3 import Mystem\n",
        "from string import punctuation\n",
        "\n",
        "mystem = Mystem()\n",
        "\n",
        "russian_stopwords = stopwords.words(\"russian\")\n",
        "russian_stopwords.extend(['...', '\"'])\n",
        "\n",
        "\n",
        "\n",
        "def lemmatize_text(text):\n",
        "  tokens = mystem.lemmatize(text.lower())\n",
        "  tokens = [token for token in tokens if token not in russian_stopwords and token != \" \"]\n",
        "  text = \" \".join(tokens)\n",
        "  return text"
      ],
      "metadata": {
        "id": "EXhoaDLWUngP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prep_text = [remove_multiple_spaces(remove_numbers(remove_punctuation(text.lower()))) for text in tqdm(df_train['title'])]"
      ],
      "metadata": {
        "id": "1RnOEQNjW4s4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(prep_text)"
      ],
      "metadata": {
        "id": "x6vu0nN4aUVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prep_text[0]"
      ],
      "metadata": {
        "id": "9qm12_xDajwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['text_pred'] = prep_text"
      ],
      "metadata": {
        "id": "HPJfCqt4alQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()"
      ],
      "metadata": {
        "id": "PkE5VDXCb_7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Стемминг***"
      ],
      "metadata": {
        "id": "2P1nOMufcKcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"russian\")"
      ],
      "metadata": {
        "id": "V7091uoucBYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "russian_stopwords = stopwords.words(\"russian\")\n",
        "russian_stopwords.extend(['т.д.', 'т', 'д'])"
      ],
      "metadata": {
        "id": "bGcsUB9scfk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "\n",
        "stemmed_texts_list = []\n",
        "for text in tqdm(df_train['text_pred']):\n",
        "    tokens = word_tokenize(text)    \n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens if token not in russian_stopwords]\n",
        "    text = \" \".join(stemmed_tokens)\n",
        "    stemmed_texts_list.append(text)\n",
        "\n",
        "df_train['text_stem'] = stemmed_texts_list"
      ],
      "metadata": {
        "id": "dgTJPgJDcoHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()"
      ],
      "metadata": {
        "id": "mFf2UV0cc5Ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words(text):\n",
        "    tokens = word_tokenize(text) \n",
        "    tokens = [token for token in tokens if token not in russian_stopwords and token != ' ']\n",
        "    return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "TarlwFQtdCGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "\n",
        "sw_texts_list = []\n",
        "for text in tqdm(df_train['text_pred']):\n",
        "    tokens = word_tokenize(text)    \n",
        "    tokens = [token for token in tokens if token not in russian_stopwords and token != ' ']\n",
        "    text = \" \".join(tokens)\n",
        "    sw_texts_list.append(text)\n",
        "\n",
        "df_train['text_sw'] = sw_texts_list"
      ],
      "metadata": {
        "id": "aPSIgcbqdqWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()"
      ],
      "metadata": {
        "id": "vc_w0U6adwyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *** Лемматизация***"
      ],
      "metadata": {
        "id": "p2YNMbYBf14f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "lemm_texts_list = []\n",
        "for text in tqdm(df_train['text_sw']):\n",
        "    #print(text)\n",
        "    try:\n",
        "        text_lem = mystem.lemmatize(text)\n",
        "        tokens = [token for token in text_lem if token != ' ' and token not in russian_stopwords]\n",
        "        text = \" \".join(tokens)\n",
        "        lemm_texts_list.append(text)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "    \n",
        "df_train['text_lemm'] = lemm_texts_list\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "SN0dq8kydysX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_train['text_sw']\n",
        "y = df_train['is_fake']"
      ],
      "metadata": {
        "id": "8edMrUMvgA4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)"
      ],
      "metadata": {
        "id": "jnuH7yhFjev1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_tags = df_train['is_fake'].unique()\n",
        "my_tags"
      ],
      "metadata": {
        "id": "VKkPrWTDjihD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Naive Bayes Classifier***"
      ],
      "metadata": {
        "id": "ZaGK3btAjpec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "gHzpIX7qjlhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb = Pipeline([('vect', CountVectorizer()),\n",
        "               ('tfidf', TfidfTransformer()),\n",
        "               ('clf', MultinomialNB()),\n",
        "              ])"
      ],
      "metadata": {
        "id": "oEwRvTrRjsCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "nb.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "sz2mHIwejtgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "y_pred = nb.predict(X_test)"
      ],
      "metadata": {
        "id": "JNiUZtMEjvGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "qB6b_1UBjz9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression"
      ],
      "metadata": {
        "id": "X_ZekyJYlggn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logreg = Pipeline([('vect', CountVectorizer()),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
        "               ])"
      ],
      "metadata": {
        "id": "God4VFAcj18e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "logreg.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "89Bk-Sk8kD2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "y_pred = logreg.predict(X_test)"
      ],
      "metadata": {
        "id": "CEJDkWWqljYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "gxFCyjCflktF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Support Vector Machine"
      ],
      "metadata": {
        "id": "xYYuuRq0lrhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "sgd = Pipeline([('vect', CountVectorizer()),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
        "               ])"
      ],
      "metadata": {
        "id": "Z17Zr17xlnMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "sgd.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "BnHsdb-Sls5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "y_pred = sgd.predict(X_test)"
      ],
      "metadata": {
        "id": "YTZXy6MyluHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "Q1QxWKrblxYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "isBNQV5Glzzl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}